{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173db67a-529f-46c2-9d64-dd2cd732b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da29c84-ee6a-411f-b560-69c2aa275f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+-------+\n",
      "| id|first_name|last_name|age|country|\n",
      "+---+----------+---------+---+-------+\n",
      "|  1|     Darcy| Phillips| 24|     YE|\n",
      "|  2|    Amelia|   Wright| 66|     CN|\n",
      "|  3|     Haris|    Ellis| 61|     CR|\n",
      "|  4|      Tony|     Hall| 51|     JO|\n",
      "|  5|     Rubie|  Stewart| 27|     RO|\n",
      "|  6|     Miley|    Perry| 27|     ZA|\n",
      "|  7|    Marcus|   Carter| 66|     CN|\n",
      "|  8|   Charlie|   Harris| 22|     SR|\n",
      "|  9|     Honey|   Rogers| 60|     IL|\n",
      "| 10|      Luke|   Harris| 66|     IR|\n",
      "| 11|     Spike|   Murphy| 57|     IN|\n",
      "| 12|   Vincent|    Adams| 51|     CN|\n",
      "| 13|     James|   Barnes| 56|     HK|\n",
      "| 14|    George|   Bailey| 18|     AT|\n",
      "| 15|    Sienna|   Holmes| 48|     ZW|\n",
      "| 16|  Isabella|  Elliott| 47|     CO|\n",
      "| 17|   Freddie|   Martin| 47|     SC|\n",
      "| 18|      Kate|   Wright| 51|     BE|\n",
      "| 19|    Albert|    Myers| 24|     BM|\n",
      "| 20|    Connie|    Wells| 64|     SC|\n",
      "+---+----------+---------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_accounts = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .load('./data/accounts.csv')\n",
    "\n",
    "df_accounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff04b8a4-adb2-4d3c-9bc3-212e5f51c93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accounts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b01769f5-5eb4-44fa-b865-45577ce04650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|country_full_name|abbreviation|\n",
      "+-----------------+------------+\n",
      "|        Argentina|          AR|\n",
      "|        Australia|          AU|\n",
      "|          Austria|          AT|\n",
      "|          Bahamas|          BS|\n",
      "|          Bahrain|          BH|\n",
      "|       Bangladesh|          BD|\n",
      "|         Barbados|          BB|\n",
      "|          Belgium|          BE|\n",
      "|           Belize|          BZ|\n",
      "|            Benin|          BJ|\n",
      "|          Bermuda|          BM|\n",
      "|          Bolivia|          BO|\n",
      "|           Brazil|          BR|\n",
      "|         Bulgaria|          BG|\n",
      "|     Burkina Faso|          BF|\n",
      "|            Chile|          CL|\n",
      "|            China|          CN|\n",
      "|         Colombia|          CO|\n",
      "|       Costa Rica|          CR|\n",
      "|   CÃ´te D' Ivoire|          CI|\n",
      "+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_abbreviation = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\",\";\") \\\n",
    "    .load('./data/country_abbreviation-original.csv')\n",
    "\n",
    "df_country_abbreviation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f2ad3e-4154-4ba8-900b-278145d55069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_abbreviation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ddd14ea-90a3-468d-9f8c-71145a87416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------------+-------+\n",
      "|    id| amount|account_type|transaction_date|country|\n",
      "+------+-------+------------+----------------+-------+\n",
      "|179528|-730.86|    Business|      2013-07-10|     SV|\n",
      "|378343|-946.98|    Personal|      2018-04-06|     YE|\n",
      "| 75450|7816.92|Professional|      2016-11-20|     SI|\n",
      "|357719| 704.02|    Business|      2016-11-06|     ID|\n",
      "|110511| 3462.6|    Personal|      2018-01-18|     BS|\n",
      "|461830| 762.81|Professional|      2017-06-20|     CN|\n",
      "| 30180|5390.24|Professional|      2021-05-26|     GN|\n",
      "| 65398|4765.77|    Personal|      2018-05-01|     TR|\n",
      "|170899|8775.89|    Business|      2013-10-16|     SK|\n",
      "|234300|8455.18|Professional|      2015-10-06|     LU|\n",
      "|208027| 6244.1|    Business|      2020-03-06|     AE|\n",
      "|161212|5904.56|    Personal|      2016-09-07|     EG|\n",
      "|105372|4079.76|Professional|      2015-02-12|     MT|\n",
      "|205321| 3570.4|Professional|      2012-07-02|     MU|\n",
      "|410863|2328.83|    Business|      2012-12-20|     SR|\n",
      "|486752| 5454.8|Professional|      2015-02-10|     CU|\n",
      "|208564|8695.17|    Personal|      2013-01-03|     IT|\n",
      "|196682|-905.87|    Personal|      2019-01-28|     HU|\n",
      "|491196|8781.02|Professional|      2017-05-11|     IR|\n",
      "|108286|3485.95|    Personal|      2011-12-13|     ZW|\n",
      "+------+-------+------------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\").option(\"delimiter\",\";\") \\\n",
    "    .load('./data/transactions-original.csv')\n",
    "\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f80bfbf-4900-475f-8226-b72bb23dd3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970df180-3c99-4a38-bf8b-9f8247450194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.createOrReplaceTempView(\"transactions\")\n",
    "df_country_abbreviation.createOrReplaceTempView(\"country_abbreviation\")\n",
    "df_accounts.createOrReplaceTempView(\"accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9d039c4-28a3-49b8-aa5d-1a306083cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|account_type|account_type_count|\n",
      "+------------+------------------+\n",
      "|    Personal|           1667072|\n",
      "|Professional|           1667358|\n",
      "|    Business|           1665570|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many accounts of each type there are using Spark SQL. The return type is a\n",
    "# dataframe [account_type: string, account_type_count: int]\n",
    "\n",
    "aggr_df = spark.sql(\"\"\"\n",
    "    SELECT account_type, count(*) as account_type_count \n",
    "    FROM transactions \n",
    "    GROUP BY account_type\n",
    "\"\"\")\n",
    "\n",
    "aggr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31d40a22-3d55-4c4a-834a-fb4a4760ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|    id|       sum(amount)|\n",
      "+------+------------------+\n",
      "|482333|          27174.07|\n",
      "|222048|          48004.81|\n",
      "|328078|          36948.25|\n",
      "|192401|          36736.98|\n",
      "|273916| 47475.37999999999|\n",
      "|485103|          62198.93|\n",
      "|300282|          55103.62|\n",
      "| 20683|          56448.72|\n",
      "| 15846|          58671.91|\n",
      "|446783| 98085.51000000001|\n",
      "| 92182|           42335.3|\n",
      "|477485|          22114.03|\n",
      "|171142|40428.899999999994|\n",
      "|317762|          40025.55|\n",
      "| 65478| 57941.90000000001|\n",
      "|306768|          26566.93|\n",
      "|380411|          43652.94|\n",
      "|304681|          37827.69|\n",
      "|475638|44509.100000000006|\n",
      "| 97413|          39611.24|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate only the balance and the latest date for each account from transactions.csv. \n",
    "# To calculate the balance, summarize all the transactions for each account. The return type is\n",
    "# a dataframe [account_id: string, balance: string, latest_date: date].\n",
    "\n",
    "balance_df = spark.sql(\"\"\"\n",
    "    SELECT id, SUM(amount) \n",
    "    FROM transactions \n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "balance_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd957d95-7321-40be-a8ce-051a4d096c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------------+-------+\n",
      "|    id| amount|account_type|transaction_date|country|\n",
      "+------+-------+------------+----------------+-------+\n",
      "|482333|7269.27|    Business|      2012-08-18|     ID|\n",
      "|482333| 123.38|    Personal|      2013-09-01|     SE|\n",
      "|482333|5856.55|    Business|      2015-11-17|     PT|\n",
      "|482333|2049.52|    Personal|      2016-07-22|     LB|\n",
      "|482333|-609.49|    Personal|      2016-11-10|     VN|\n",
      "|482333|  -32.0|Professional|      2016-12-24|     HR|\n",
      "|482333|3967.39|    Personal|      2019-03-03|     IQ|\n",
      "|482333| 2049.5|    Personal|      2019-09-06|     BO|\n",
      "|482333|6499.95|Professional|      2020-07-17|     ID|\n",
      "+------+-------+------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions \n",
    "    WHERE id = 482333\n",
    "    ORDER BY transaction_date\n",
    "\"\"\")\n",
    "\n",
    "ddf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34947486-7ccd-4c48-a607-aff8ef8f1da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|min(transaction_date)|max(transaction_date)|\n",
      "+---------------------+---------------------+\n",
      "|           2012-08-18|           2020-07-17|\n",
      "+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf2 = spark.sql(\"\"\"\n",
    "    SELECT MIN(transaction_date), MAX(transaction_date)\n",
    "    FROM transactions \n",
    "    WHERE id = 482333\n",
    "\"\"\")\n",
    "\n",
    "ddf2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30ab1177-18e0-4255-bbbe-1b89c86a9601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+---------------------+\n",
      "|    id|min(transaction_date)|max(transaction_date)|\n",
      "+------+---------------------+---------------------+\n",
      "|482333|           2012-08-18|           2020-07-17|\n",
      "|222048|           2011-07-22|           2020-07-20|\n",
      "|328078|           2011-10-09|           2020-02-01|\n",
      "|192401|           2011-05-06|           2020-01-30|\n",
      "|273916|           2012-04-09|           2021-05-30|\n",
      "|485103|           2011-02-17|           2021-05-22|\n",
      "|300282|           2012-04-28|           2021-05-01|\n",
      "| 20683|           2011-05-16|           2021-10-27|\n",
      "| 15846|           2013-01-18|           2020-12-23|\n",
      "|446783|           2011-05-11|           2021-12-11|\n",
      "| 92182|           2013-03-14|           2020-08-08|\n",
      "|477485|           2011-05-23|           2020-05-23|\n",
      "|171142|           2012-05-01|           2021-04-07|\n",
      "|317762|           2011-08-22|           2021-12-02|\n",
      "| 65478|           2011-02-14|           2021-10-06|\n",
      "|306768|           2011-12-04|           2019-12-19|\n",
      "|380411|           2011-12-11|           2020-06-02|\n",
      "|304681|           2013-07-31|           2021-03-26|\n",
      "|475638|           2013-06-02|           2021-11-23|\n",
      "| 97413|           2012-05-08|           2018-05-01|\n",
      "+------+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf3 = spark.sql(\"\"\"\n",
    "    SELECT id, MIN(transaction_date), MAX(transaction_date)\n",
    "    FROM transactions t\n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "ddf3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d15d6a4-fdc1-4adf-8541-69d103e06d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------+\n",
      "| id|           balance|  max_date|\n",
      "+---+------------------+----------+\n",
      "|  1| 51909.75000000001|2021-08-19|\n",
      "|  2|58346.670000000006|2021-02-25|\n",
      "|  3|          33709.88|2020-05-04|\n",
      "|  4|30160.090000000004|2021-01-06|\n",
      "|  5|          34593.07|2020-05-31|\n",
      "|  6|          55611.34|2021-07-01|\n",
      "|  7|45952.439999999995|2021-09-08|\n",
      "|  8|          40101.59|2020-05-14|\n",
      "|  9|          46625.15|2019-01-03|\n",
      "| 10|          50807.19|2020-12-27|\n",
      "| 11|          25039.38|2021-08-24|\n",
      "| 12|32998.329999999994|2021-05-16|\n",
      "| 13|          39822.72|2021-09-02|\n",
      "| 14|          35763.46|2020-03-17|\n",
      "| 15|39481.770000000004|2021-11-07|\n",
      "| 16|          52134.41|2017-07-16|\n",
      "| 17|23278.489999999998|2021-12-14|\n",
      "| 18|26161.980000000003|2020-02-27|\n",
      "| 19| 43046.28999999999|2021-05-27|\n",
      "| 20|          41532.67|2020-11-15|\n",
      "+---+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "balance_df = spark.sql(\"\"\"\n",
    "    WITH sum_balance AS (\n",
    "        SELECT id, SUM(amount) as balance\n",
    "        FROM transactions \n",
    "        GROUP BY id\n",
    "    ), \n",
    "    maxd AS (\n",
    "        SELECT id, MAX(transaction_date) as max_date\n",
    "        FROM transactions t\n",
    "        GROUP BY id\n",
    "    )\n",
    "    SELECT DISTINCT t.id, sb.balance, md.max_date\n",
    "    FROM transactions t\n",
    "    JOIN sum_balance sb ON t.id = sb.id\n",
    "    JOIN maxd md ON md.id = sb.id\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "balance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08ecabcb-5b88-4a8f-bbbe-e6f0d7925ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+---------------------+\n",
      "| id|min(transaction_date)|max(transaction_date)|\n",
      "+---+---------------------+---------------------+\n",
      "|  1|           2011-04-08|           2021-08-19|\n",
      "|  2|           2012-01-30|           2021-02-25|\n",
      "+---+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf4 = spark.sql(\"\"\"\n",
    "    SELECT id, MIN(transaction_date), MAX(transaction_date)\n",
    "    FROM transactions t\n",
    "    WHERE id = 1 or id = 2\n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "ddf4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "153dce91-6da8-402e-b8d8-f832f30cb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|       sum(amount)|\n",
      "+---+------------------+\n",
      "|  1| 51909.75000000001|\n",
      "|  2|58346.670000000006|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = spark.sql(\"\"\"\n",
    "    SELECT id, SUM(amount) \n",
    "    FROM transactions \n",
    "    WHERE id = 1 or id = 2\n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "df5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1ab61-4c07-4591-9fd7-ca5edfc6773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W34  21.08.23 \n",
    "Spark API \n",
    "2. Write a function using Spark Python or Spark Scala API to calculate total earnings (sum of\n",
    "transactions above 0) for each user from Switzerland by year as a pivot table. \n",
    "\n",
    "The result dataframe should contain:\n",
    "- user full names as one field split by whitespace, years, and earning values.\n",
    "\n",
    "Note: The real datasets could be more extensive than provided csv samples.\n",
    "\n",
    "3. Store the AI chat conversation in the \"Task_2.txt\" file and commit it to the repository. If you\n",
    "did without AI support, create an empty file.\n",
    "\n",
    "4. Commit C#4 with the \"UC#33 Task_2\" message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d506037f-b51a-4175-96a2-dcd44e75d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|country_full_name|abbreviation|\n",
      "+-----------------+------------+\n",
      "|      Switzerland|          CH|\n",
      "+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf6 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM country_abbreviation\n",
    "    WHERE country_full_name = 'Switzerland'\n",
    "\"\"\")\n",
    "\n",
    "ddf6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4cbb52f-0bc6-4610-ae1d-b055f5ac4c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|    id|sum(amount)|\n",
      "+------+-----------+\n",
      "|283980|    2612.92|\n",
      "|  6357|    9826.82|\n",
      "|226243|    9103.32|\n",
      "| 44437|    2909.02|\n",
      "| 81501|    2364.32|\n",
      "|145504|    1989.41|\n",
      "|251742|    8526.42|\n",
      "|341687|    8949.24|\n",
      "| 30903|    9668.23|\n",
      "|453490|    7411.66|\n",
      "|  7240|    1485.11|\n",
      "| 89874|    6253.26|\n",
      "| 47217|    8946.42|\n",
      "|284346|    9731.56|\n",
      "|420759|    8314.65|\n",
      "|376414|     5606.8|\n",
      "|474711|     8189.4|\n",
      "|346791|    1983.52|\n",
      "|188488|     979.04|\n",
      "|454924|    2795.71|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf7 = spark.sql(\"\"\"\n",
    "    SELECT t.id, SUM(amount)\n",
    "    FROM country_abbreviation ca\n",
    "    JOIN transactions t ON t.country = ca.abbreviation\n",
    "    WHERE country_full_name = 'Switzerland'\n",
    "    GROUP BY id\n",
    "    HAVING SUM(amount) > 0 \n",
    "\"\"\")\n",
    "\n",
    "ddf7.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77faabd2-b5b6-4456-a1f8-95b3abfbb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----------+\n",
      "| id|transaction_date|sum(amount)|\n",
      "+---+----------------+-----------+\n",
      "|  8|      2015-09-29|    3481.48|\n",
      "| 19|      2017-01-04|    5630.04|\n",
      "| 20|      2020-03-16|    -432.68|\n",
      "| 48|      2020-09-09|    7814.77|\n",
      "| 51|      2020-12-21|    8011.46|\n",
      "| 63|      2019-09-11|    -774.16|\n",
      "| 66|      2021-07-18|    6961.53|\n",
      "| 76|      2019-08-12|    8116.98|\n",
      "| 76|      2020-03-20|      204.7|\n",
      "| 77|      2020-08-05|     1298.8|\n",
      "| 80|      2011-09-29|    1074.49|\n",
      "| 86|      2013-11-12|      673.9|\n",
      "| 98|      2012-07-20|    9278.56|\n",
      "|101|      2012-09-19|    -882.35|\n",
      "|104|      2021-07-29|     705.88|\n",
      "|121|      2020-06-11|    1230.22|\n",
      "|132|      2012-03-07|    4971.14|\n",
      "|136|      2015-07-24|    8145.67|\n",
      "|145|      2019-01-11|    1801.25|\n",
      "|160|      2021-07-28|    5702.53|\n",
      "+---+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf8 = spark.sql(\"\"\"\n",
    "    SELECT t.id, t.transaction_date, SUM(amount)\n",
    "    FROM country_abbreviation ca\n",
    "    JOIN transactions t ON t.country = ca.abbreviation\n",
    "    WHERE country_full_name = 'Switzerland'\n",
    "    GROUP BY id, transaction_date \n",
    "    ORDER BY id\n",
    "\"\"\")\n",
    "\n",
    "ddf8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c72c5fce-cbb8-47a7-ad95-198e4be5c48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "| id|first_name|last_name|\n",
      "+---+----------+---------+\n",
      "|  1|     Darcy| Phillips|\n",
      "|  2|    Amelia|   Wright|\n",
      "|  3|     Haris|    Ellis|\n",
      "|  4|      Tony|     Hall|\n",
      "|  5|     Rubie|  Stewart|\n",
      "|  6|     Miley|    Perry|\n",
      "|  7|    Marcus|   Carter|\n",
      "|  8|   Charlie|   Harris|\n",
      "|  9|     Honey|   Rogers|\n",
      "| 10|      Luke|   Harris|\n",
      "| 11|     Spike|   Murphy|\n",
      "| 12|   Vincent|    Adams|\n",
      "| 13|     James|   Barnes|\n",
      "| 14|    George|   Bailey|\n",
      "| 15|    Sienna|   Holmes|\n",
      "| 16|  Isabella|  Elliott|\n",
      "| 17|   Freddie|   Martin|\n",
      "| 18|      Kate|   Wright|\n",
      "| 19|    Albert|    Myers|\n",
      "| 20|    Connie|    Wells|\n",
      "+---+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf9 = spark.sql(\"\"\"\n",
    "    SELECT id,\tfirst_name,\tlast_name\n",
    "    FROM accounts\n",
    "\"\"\")\n",
    "\n",
    "ddf9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1a8300e-1b56-43b8-ac2d-bc22f8f382a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+----------------+-------+\n",
      "| id| amount|account_type|transaction_date|country|\n",
      "+---+-------+------------+----------------+-------+\n",
      "|  8|7685.97|    Personal|      2020-05-14|     LU|\n",
      "|  8|8150.49|Professional|      2012-01-20|     PK|\n",
      "|  8|6095.47|Professional|      2019-12-27|     BO|\n",
      "|  8|4904.49|    Business|      2012-07-15|     FR|\n",
      "|  8|3511.23|Professional|      2012-05-24|     LR|\n",
      "|  8|3481.48|Professional|      2015-09-29|     CH|\n",
      "|  8|6272.46|    Personal|      2013-11-16|     BB|\n",
      "+---+-------+------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf10 = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM transactions t\n",
    "    WHERE id = 8 \n",
    "\"\"\")\n",
    "\n",
    "ddf10.show()\n",
    "\n",
    "# +---+-------+------------+----------------+-------+\n",
    "# | id| amount|account_type|transaction_date|country|\n",
    "# +---+-------+------------+----------------+-------+\n",
    "# |  8|3481.48|Professional|      2015-09-29|     CH|\n",
    "# +---+-------+------------+----------------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "736bb48d-c7b5-4f48-82c2-2b80bdb7c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------+\n",
      "| id|transaction_date|country|\n",
      "+---+----------------+-------+\n",
      "|  8|      2015-09-29|     CH|\n",
      "| 19|      2017-01-04|     CH|\n",
      "| 20|      2020-03-16|     CH|\n",
      "| 48|      2020-09-09|     CH|\n",
      "| 51|      2020-12-21|     CH|\n",
      "| 63|      2019-09-11|     CH|\n",
      "| 66|      2021-07-18|     CH|\n",
      "| 76|      2020-03-20|     CH|\n",
      "| 76|      2019-08-12|     CH|\n",
      "| 77|      2020-08-05|     CH|\n",
      "| 80|      2011-09-29|     CH|\n",
      "| 86|      2013-11-12|     CH|\n",
      "| 98|      2012-07-20|     CH|\n",
      "|101|      2012-09-19|     CH|\n",
      "|104|      2021-07-29|     CH|\n",
      "|121|      2020-06-11|     CH|\n",
      "|132|      2012-03-07|     CH|\n",
      "|136|      2015-07-24|     CH|\n",
      "|145|      2019-01-11|     CH|\n",
      "|160|      2021-07-28|     CH|\n",
      "+---+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ddf11 = spark.sql(\"\"\"\n",
    "    SELECT id, transaction_date, country \n",
    "    FROM transactions t\n",
    "    WHERE id IN ( 8,  19,  20,  48,  51,  63,  66,  76,  76,  77,  80,  86,  98, 101, 104, 121, 132, 136, 145, 160) \n",
    "        AND COUNTRY = 'CH'\n",
    "    ORDER BY id\n",
    "\"\"\")\n",
    "\n",
    "ddf11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c37d310-76f5-445f-bd5b-838704971eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+-------+\n",
      "|first_name|last_name|transaction_date|    AMT|\n",
      "+----------+---------+----------------+-------+\n",
      "|    Miller|    Grant|      2017-12-20|8969.46|\n",
      "|     Daryl| Richards|      2017-08-30|3218.19|\n",
      "|      Maya|    Perry|      2011-05-15|6686.35|\n",
      "|     Daisy|    Clark|      2014-08-09|3932.69|\n",
      "|    Arthur|Armstrong|      2018-12-29|1842.69|\n",
      "| Annabella|  Higgins|      2011-07-28|1171.24|\n",
      "|    Wilson| Phillips|      2013-10-27| 1960.1|\n",
      "|   Vincent|  Johnson|      2011-12-02| 9490.7|\n",
      "|    Carlos|    Allen|      2018-11-26|9485.89|\n",
      "|     Sofia| Johnston|      2013-01-06|9729.65|\n",
      "|    Vivian|    Grant|      2018-07-18|5106.76|\n",
      "|    Oliver| Ferguson|      2018-06-29|1106.76|\n",
      "|      Adam|   Brooks|      2018-12-07|4130.63|\n",
      "|    Arnold|   Harris|      2011-10-17|9708.46|\n",
      "|  Isabella|    Jones|      2016-03-03|7111.15|\n",
      "|    Kellan|    Smith|      2018-04-06|6269.93|\n",
      "|   Richard|   Howard|      2018-08-25|5215.36|\n",
      "|      Lana|    Craig|      2016-05-18|3300.32|\n",
      "|    Victor|    Adams|      2018-01-12|3295.33|\n",
      "|      Alen| Mitchell|      2021-11-24|5280.74|\n",
      "+----------+---------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf12 = spark.sql(\"\"\"\n",
    "    WITH us AS (\n",
    "        SELECT a.id,\ta.first_name,\ta.last_name\n",
    "        FROM accounts a\n",
    "    ), trrr (\n",
    "        SELECT t.id, t.transaction_date, SUM(amount) as AMT\n",
    "        FROM country_abbreviation ca\n",
    "        JOIN transactions t ON t.country = ca.abbreviation\n",
    "        JOIN us u ON u.id = t.id\n",
    "        WHERE country_full_name = 'Switzerland'\n",
    "        GROUP BY t.id, t.transaction_date \n",
    "        HAVING SUM(amount) > 0\n",
    "        ORDER BY t.id\n",
    "    )    \n",
    "    SELECT u.first_name, u.last_name, tr.transaction_date, tr.AMT\n",
    "    FROM trrr tr \n",
    "    JOIN us u ON tr.id = u.id \n",
    "\"\"\")\n",
    "\n",
    "ddf12.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f35b88-eb7d-459b-90b2-e1e5bed17deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf13  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
